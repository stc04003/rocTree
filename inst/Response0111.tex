\documentclass[12pt]{article}

\usepackage{amssymb,amsfonts,latexsym,epsfig, natbib, rotating, graphicx,titlesec, epstopdf, url, hyperref,threeparttable, color, soul,amsmath,subcaption}

\raggedbottom \textwidth=6.5in \textheight=9in \oddsidemargin=0.1in
%\evensidemargin=.0in
\headheight=-1in
\parskip=12pt
\parindent=0.15in   
\def\thesection {\arabic{section}.}
\def\thesubsection {\arabic{section}.\arabic{subsection}}
 
\baselineskip=0.5cm
 
%-----------------------------------------------------------------------%
% My Abreviations

\newtheorem{theorem}{Theorem} 
\newtheorem{remark}{Remark}
   \newcommand{\CON}{{\rm CON}}
   \newcommand{\ROC}{{\rm ROC}}
   \newcommand{\ICON}{{\rm ICON}}
   \newcommand{\FPR}{{\rm FPR}}
    \newcommand{\TPR}{{\rm TPR}}
   \newcommand{\bmB}{ {\bm B} } 
   \newcommand{\bmG}{ {\bm G} }
   \newcommand{\bmH}{ {\bm H} } 
   \newcommand{\bmL}{ {\bm L} } 
   \newcommand{\bmQ}{ {\bm Q} }
   \newcommand{\bmGam}{ {\bm \Gamma} }
   \newcommand{\T}{\mathcal{T}}
   \newcommand{\mZ}{{\mathcal Z}}
   \newcommand{\bmx}{ {\bm x} } 
   \newcommand{\bmz}{ {\bm z} } 
   \newcommand{\bmw}{ {\bm w} } 
   \newcommand{\bmX}{ {\bm X} } 
   \newcommand{\bmZ}{ {\bm Z} } 
   \newcommand{\bmW}{ {\bm W} } 
   \newcommand{\bmS}{ {\bm S} } 
   \newcommand{\bms}{ {\bm s} } 
   \newcommand{\bmU}{ {\bm U} }    
   \newcommand{\bmJ}{ {\bm J} } 
   \newcommand{\bmK}{ {\bm K} } 
   \newcommand{\bmPhi}{ {\bm \Phi} } 

   \newcommand{\bmb}{ {\bm \beta} } 
   \newcommand{\bmO}{ {\bm \Omega} } 
   \newcommand{\pr}{ \mbox{pr} } 
   \newcommand{\bmeta}{ {\bm \eta} } 
   \newcommand{\bmxi}{ {\bm \xi} } 
   \newcommand{\bmPsi}{ {\bm \Psi} } 
   \newcommand{\bmth}{ {\bm \theta} } 
   \newcommand{\bmsig}{ {\bm \Sigma} } 

   \newcommand{\bmkappa}{ {\bm \kappa} } 
  \newcommand{\bm}[1]{ \mbox{\boldmath $ #1 $} }
  \newcommand{\blue}[1]{#1}
  \newcommand{\red}[1]{\textcolor{red}{#1}}
  \newcommand{\green}[1]{\textcolor{green}{#1}}
  \newcommand{\cy}[1]{\textcolor{magenta}{#1}}
  \newcommand{\vp}{\vspace{0.1in}}
\newcommand{\ns}{\normalsize}
\newcommand{\fn}{\footnotesize}
\newcommand{\scr}{\scriptsize}
\DeclareMathOperator{\dev}{d\!}
\pdfminorversion=4
\begin{document}
%----------------------------------------------
 \parindent = 0em
% \noindent \textbf{JASA-T&M-2015-0053\\Response to Reviewers' Comments:}\\
  \noindent \textbf{Reply to Reviews (BIOM2018393M)}\\

\noindent{\blue{We thank the Editor, the Associate Editor and three referees for their helpful and constructive comments. In this revision, we have carefully revised the manuscript to address the comments in the review. Several paragraphs in the manuscripts have been rewritten for improved readability, and more simulations and analysis have been conducted. Below please find our point-to-point response to reviewers' comments.}}
 
 \noindent \textbf{Response to Editor's comments}


\emph{Your paper BIOM2018393M, entitled ``ROC-Guided Survival Trees and Forests,'' has been reviewed by an associate editor and three referees, whose reports are enclosed.  Based upon these reports, I am happy to inform you that we would be willing to consider a revision.  However, I must warn you that considerable work needs to be done before we can move towards final acceptance. The reviews of your paper were quite mixed, with two reviewers being generally positive about the work and one reviewer recommending rejection. While we all agree that this is interesting and potentially important work, there were several concerns.}


	
\emph{There were concerns about the presentation of the work. One reviewer notes for instance that the authors need to do more to introduce their topic properly to the reader. The authors should not just assume that the reader is intimately familiar with all of the reference material.  Of course, the article can't possibly recapitulate all previous research so choices for brevity must be made.  But, there needs to be an intelligent balance between helping the reader understand with content and references vs. providing references only. 
}
	
\noindent{\blue{\textbf{Response}: We appreciate your comments. In this revision, we have expanded the discussion of existing works on survival trees and forests in Section 1, page \cy{1--3}. The content of several key works and recent works are briefly introduced.}}

\emph{There were also important methodological concerns in that there is a disconnect between the theory and what's being proposed. One reviewer notes for instance that the most important section (3.1) which discusses the ROC splitting rule is skimmed over. Practical issues with this rule are that its heavily intensive and requires repeated estimation of unstable quantities that only get worse as the tree gets deeper. One of these quantities will be estimated using kernel density estimation but this is questionable. This is because data is not independent within a tree node so even bandwidth selection will be one problem to contend with.
}

\noindent{\blue{\textbf{Response}: Following your suggestion, we have expanded the discussion of the ROC-based splitting rule. 
\begin{enumerate}
	\item We clarify the computational cost. At each time $t$, the concordance probability of a partition-based risk score can be decomposed into sums of node-level estimates (Equation (\cy{7})), thus the computation cost is much lower than the usual estimation procedure based on a U-statistic. We note that the computational cost also depends on number of time points considered in the global measure, and a moderate number gives reasonably good performance. A discussion is added on page \cy{12}. Moreover, our second splitting rule (page \cy{14}) can be written as a weighted average of absolute difference between hazards of two child nodes, thus does not require intensive computation.
	\item The kernel type estimators can result in biased estimation when the terminal nodes contain a small number of observations. To solve this issue, we control the node sizes by a pruning procedure as well as a stopping rule that sets the minimum node size. Thus a large tree with small node sizes is not very likely to be selected as the final model. We have expanded the discussion on pruning in Section 3. In practice, if the true model is complex and a very large tree is needed to fit the data, we recommend the use of random forest for stable prediction. A discussion is added on page \cy{25}. 
	\item Our consistency result of survival trees is derived under the conditional independent censoring assumption (i.e., $P(t\le T<t+\dev t\mid Z(t)=z,T\ge t, C\ge t) = \lambda(t\mid z)\dev t$), which is along the same line with existing works on consistency of survival trees \citep{butler1989tree}. In practice, with a finite number of terminal nodes, conditional independent censoring can result in independently non-identically distributed sample structure and dependent censoring within each node.
	For estimation of a tree with a finite number of terminal nodes, our discussion in Section 2 and 3 is under stronger censoring assumptions, so that the node-specific estimate is valid. We note that this problem also exists in previous works of survival trees. As a solution, our methods can be extended to handle conditionally independent censoring by modeling the conditional censoring distribution \citep{steingrimsson2018censoring}. Due to space limit, the details are given in Supplementary Materials.
	\item We have added a remark (Remark 3, page \cy{15}) on bandwidth selection for survival trees. We also add a discussion on bandwidth selection for forests on page \cy{18}. Simulation studies have demonstrated reasonably good performance of the proposed bandwidth selection method.
\end{enumerate}}}
	

\emph{There were further concerns about the empirical results. In particular, you use time dependent simulations and then compare your method to usual random forest methods that do not accommodate time dependence. One reviewer recommends that you compare to a time dependent Cox regression, which is easily implemented using standard approaches for evaluating prediction error in survival problems.
}

\noindent{\blue{\textbf{Response}: In this revision, we have revised the simulation studies and included the comparison with a Cox model with time-dependent covariates.}}


\emph{Other comments}

\emph{	- p10. Define the independence symbol.
}	

\noindent{\blue{\textbf{Response}: We have revised the sentence to ``{For ease of discussion, we assume  $C$ is independent of $\{ T, Z(\cdot)\}$ in Section 3.2''.}}}

\emph{ - Second sentence below proposition 2 starts in an unclear way.
}

\noindent{\blue{\textbf{Response}: We have revised the sentence to be ``When \blue{$\widehat{\omega}(\cdot)>0$}, the equality holds if and only if $\widehat{\lambda}(\cdot\mid\tau_1^L) = \widehat{\lambda}(\cdot\mid\tau_1^R)$ almost everywhere on $(0,s]$''.}}

\emph{ -  The considered independent censoring assumptions seems strong (i.e. stronger than what I believe is typically needed).
}

\noindent{\blue{\textbf{Response}: The large-sample result of the survival tree requires conditionally independent censoring assumption (A1) on page 7, which is consistent with the literature. In this revision, we have moved all the conditions for Theorem 1 to the main context to improve readability. \\Note that the consistency results of survival trees do not account for the data-dependent splitting and pruning algorithm. We would like to point out that valid estimation of the splitting criteria (e.g., log-rank statistic and the proposed concordance measure) typically requires a stronger censoring assumption. An extension to handle the milder conditionally independent censoring assumption in splitting is given in the Supplementary Materials.}}





\noindent \textbf{Response to Associate Editor's comments:}

\emph{The authors propose a method for constructing survival trees and forests with time-dependent covariates. I think this work is quite interesting, but could be improved in several respects. My own comments follow, which emphasize the important points raised by the referees; in addition to this, the authors should carefully attend to the particular comments of Referee 3 in revising their manuscript.}

\emph{\textbf{Readability}}

\emph{I am in agreement with Referee 1 and Referee 3 that the paper is more difficult read than necessary. I sympathize with the authors in that it is difficult to strike an appropriate balance between ease of reading, rigor, and an appropriate level of detail. I found the presentation of the material to distract from the main claims the authors make, which I understand to be:}
\begin{enumerate}
\item \emph{Integrating a time-dependent AUC, estimated using a time-invariant partition, is a sensible thing to do, and allows us to incorporate time-dependence in a more natural manner than existing methods;
}
\item \emph{The AUC corresponds to a concordance measure, which the authors integrate against various measures to construct metrics for splitting the nodes in the tree;}

\item \emph{For stability purposes, when constructing a forest, it is better to average the estimating equations associated to the hazard/survival function of individual survival trees rather than averaging these quantities directly.}

\end{enumerate}

\noindent{\blue{\textbf{Response}: We appreciate the summary that is helpful in clarifying the contributions of the work. We revised the introduction (page \cy{4}) to emphasize the use of integral of time-dependent AUC and time-invariant partition. We also adjusted the presentation of the manuscript by first introducing the time-invariant partition and the estimation of partition-based risk function in Section 2, and moving all the materials related to how to build such a survival tree using ROC to Section 3.}}

\emph{I think more should also be said about the consequences of how to think about the use of a time-invariant partition. Decision trees are known for their interpretability, but it seems not so interpretable to think about observations which move between the terminal nodes. As presented, it is somewhat difficult to understand the consequences of such a model. I think this confusion is reflected in the comments of both Referee 1 and Referee 3.
}

\noindent{\blue{\textbf{Response}: Thanks for the comments. To address your comments, we have added a discussion of how to interpret the results on page \cy{5--6}; we also added an example for better illustration.}\\
\begin{quote}
\blue{``The time-invariant partition considered allows a sparse model and an easy interpretation of the decision rule. At each fixed time $t$, the tree partitions the survivor population based on $Z(t)$ and predicts the instantaneous failure risk. Thus the interpretation at a fixed time point is along the same line as classification and regression trees. Since the risk within each terminal node changes with time, it is essential to look at the hazard curves of each terminal node to determine the subgroups with high failure risks. Consider an example in Figure~1a where the time-invariant partition ${\T} = \{\tau_1,\tau_2 \}$ is based on a single predictor (i.e., $p=1$), $Z(t)$, and divides the survivor population into two subgroups. The root node contains the survivors at $t$; survivors with $Z(t)\le c$ belong to node $\tau_1$ and survivors with $Z(t)>c$ belong to node $\tau_2$. Figures~1b and~1c denote two possible scenarios the node-specific hazards can be defined. A larger value of $Z(t)$ is associated with higher risk if the node-specific hazards are specified in Figure~1b, while a larger value of $Z(t)$ is associated with lower risk in the early period and is associated with higher risk in the later period if the node-specific hazards are specified in Figure~1c. As the number of terminal nodes in $\mathcal{T}$ increases, the partition-based hazard function $\lambda_{\T}(t\mid z)$ approximates the true hazard function $\lambda(t\mid z)$. In Section 2.2, we show that the time-invariant rule is asymptotically valid. Compared to the Cox model that assumes multiplicative covariate effects and requires a correct choice of the functional form of the time-dependent covariates \citep{fisher1999time}, the partition-based hazard function can be more robust to model mis-specification.''}
\end{quote}

\begin{figure}[h]
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{e1_3}
		\caption{A time-invariant partition}
		\label{fig:gull}
	\end{subfigure}
 %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
	%(or a blank line to force the subfigure onto a new line)
	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=0.9\textwidth]{e2_2}
	\caption{ Case I: Node-specific hazards}
		\label{fig:tiger}
	\end{subfigure}
%add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
	%(or a blank line to force the subfigure onto a new line)
	\begin{subfigure}[b]{0.35\textwidth}
		\includegraphics[width=\textwidth]{e3_2}
		\caption{Case II: Node-specific hazards}
	\end{subfigure}
	\caption{Illustration of the survival tree and hazard prediction}\label{}
\end{figure}




\emph{I also feel that all propositions should be more self-contained than they are. It is good practice to keep theorems self-contained enough to where readers do not have to guess where in the text certain quantities are defined if they happen to have lost track of them. This should be carried out up-to the point where it does not create undue difficulty for the authors. For example, it would be easy for the authors to include the sufficient conditions in the main text.}


\noindent{\blue{\textbf{Response}: To address your comment, we have moved the sufficient conditions of Theorem 1 to the main text. We also revised Proposition 1 and 2 so that the quantities can be easily found in the text. We have also included a table on notations in the Supplementary Materials to improve readability.}}

\emph{\textbf{Computation issues}}

\emph{Not enough is said about the computational concerns with implementing this strategy. Referee 3 makes the point that concordance computations are often impeded by the fact that they, in general, require $N^2$ computations. It is unclear to me whether this is an issue or not, since I think the authors effectively only need to compute the concordance for an $2\times J$  contingency table where $J$ is the number of terminal nodes in the tree, for each tree. But then, this also needs to be done for each t when computing the required integrals (are these quick and easy to compute?), so it remains unclear exactly what the computational cost will be.}


\noindent{\blue{\textbf{Response}:  We appreciate your thorough understanding of the computation of concordance, given the lack of discussion in our previous manuscript. The usual $N^2$ computation costs for concordance measure is not a issue in our work, because the estimation of $\CON_t$, based on Equation (7), only requires node-level estimates of the $J$ terminal nodes. Moreover, the computational cost also depends on the number of time points used in the global measure. Based on our simulation studies, using a moderate number of time points such as 10 yields comparable results as using a denser grid. If researchers have limited computational resources, they can start from a less refined grid or fit the tree on a shorter time interval. A discussion is now added on page 12.}}

\begin{quote}
\blue{``Note that the usual $O(n^2)$ computational costs for a concordance measure can be reduced by using Equation (7), because the tree-based hazard ${\lambda}_{\T}(t \mid z)$ at time $t$ takes $M$ discrete values. To estimate ${\ICON}$, we use 
$\widehat{\ICON}(\widehat{\lambda}_{\T})= \int_{0}^{{s}}  \widehat{\CON}_t(\widehat{\lambda}_{\T}(t\mid \cdot))\widehat{\omega}(t)d t$,
where $\widehat{\omega}(t)$ is a weight function that possibly depends on the data. In practice, one can approximate the integral by the trapezoidal rule. Thus the computational cost also depends on the grid of $t$ used in the approximation. As demonstrated in our simulation studies, a moderate number of time points (e.g, {10}) can yield reasonably good performances.''}
\end{quote}


\emph{Related to the computational issues, I do not think enough details are presented regarding what the algorithm coming out of Section 3.1 looks like. The algorithm used should be presented so that, at a minimum, an interested reader could implement the methodology without having to guess at what the authors did; this level of detail is not even given in the supplementary material. I think just having the algorithm in the text, with a sufficient amount of detail, would go a long way to clearing up any misconceptions regarding both conceptual and computational issues the methods presented might have.}


\noindent{\blue{\textbf{Response}: To address your comment, we have moved the survival tree and forest algorithms into the text (please see Algorithm 1 and 2). More details are added to clear up potential misconceptions.}}

\emph{Another computational point which I am unclear on is that, contrary to most tree-growing algorithms I am familiar with, the tree growing algorithm the authors present is non-local. Meaning that, the concordance measure depends not just on the two child nodes and the parent node of a proposed split, but on all the other equivalence classes. This seems unappealing, as the tree you end up with may depend on the order in which the nodes are split. The authors I guess are aware of this non-locality, leading to their proposal of $\Delta$CON. If I am understanding this correctly, I think this point should be made much more clearly for the benefit of the reader; again, I think giving explicitly the algorithm, in sufficient detail, would prevent readers from having to guess about this type of thing.}


\noindent{\blue{\textbf{Response}:  Yes, the tree-growing algorithm based on overall CON is non-local }\blue{(\textbf{Following the suggestion of Reviewer 1, we have revised the notation ``CON'' to ``ICON", to emphasize the measure is an integral of $\CON_t$})}. \blue{With the ICON-based rule, we split the nodes in a sequential way. At each step, we choose a split among possible splits on \underline{all} of the current nodes, so the order of splitting depends on the data. As an alternative, one can also consider the $\Delta\ICON_\tau$-based rule (the $\Delta\CON_\tau$-based rule in the previous manuscript), which is a local splitting rule and can be implemented via recursive partition. To clarify this point, we have added the following discussion on page 13 and made this clear in Algorithm 1.}}

\begin{quote}
\blue{``We note that the splitting rule based on the $\ICON$ of the tree is non-local in the sense that the split depends not only on the data in the parent node, but also on other parts of the tree. As a result, the splits are made in a sequential way, and the order of splitting depends on the data. In what follows, we introduce another local splitting rule, where the optimal split on a node is chosen to maximize the increase of ICON within the node.''}
\end{quote}

\emph{\textbf{Stability and bandwidth estimation}}

\emph{It is unclear to me how the authors are selecting the required bandwidths, as well as what modifications the authors make to the algorithm when moving from survival trees to random forests. Random forests typically grow deeper than single trees, but the consequence of doing the kernel density estimation when constructing the deep tree becomes unclear.}

\noindent{\blue{\textbf{Response}: Thanks for the comments. In this revision, we have added a discussion of bandwidth selection. For one-sample nonparametric kernel smoothing estimator, the optimal global bandwidth on the time interval depends on the sample size and underlying distribution, thus we use different bandwidths for different nodes in the survival tree algorithm. On page 19-20, we have added the following remark.}}
%For node $\tau$, define $n_\tau = \sum_{i=1}^{n} I(Z_i(Y_i)\in\tau)$. Following standard arguments of kernel smoothing, the mean integrated square error (MISE) of the node-specific hazard estimation is of the order $O(h_\tau^4 + 1/n_\tau h_\tau)$. It can be shown that $h_{\tau}  = cn_{\tau}^{-1/5}$ will yield an MISE of the smallest order, that is, $O(n_\tau^{-4/5})$. For simplicity, we use the same $c$ for all the terminal nodes. As in Muller and Wang (1994), an possible choice of $c$ can be $s/8$, where $s$ is the length of the time interval of interest. One can also view $c$ as a tunning parameter and choose $c$ via cross-validation.}}

\begin{quote}
\blue{``We use node-specific bandwidths such that the bandwidth for node $\tau$ is $h_{\tau} = cn_{\tau}^{-1/5}$, where $n_\tau = \sum_{i=1}^n I(Z_i(Y_i)\in\tau)$. An order of $n_{\tau}^{-1/5}$ is chosen to achieve the lowest order of integrated mean square error within $\tau$. Specifically, following the arguments of existing works on smoothing hazard, it can be shown that, $\int_{0}^{s}E\{\widehat{\lambda}(t\mid \tau) -   {\lambda}(t\mid \tau)\}^2 dt  = O(h_{\tau}^4 + n_\tau^{-1} h_\tau^{-1}) = O(n_\tau^{-4/5})$. An ad hoc choice of $c$ is $c_0 = s/8$ \citep{muller1994hazard} and was used in our simulation studies. In practice, one can also choose $c$ via cross-validation.''}
\end{quote}


\noindent{\blue{Note that when using forest for hazard prediction, the bandwidth in (11) also needs to be selected. We have added the following discussion.
\begin{quote}
``For each $z$, the ensemble estimate $\widehat{\lambda}_{\mathbb T}(t\mid z)$ is a weighted kernel estimator. For $0<t \le s$, 
\begin{align*}
\widehat{\lambda}_{\mathbb T}(t\mid z) &= \sum_{i=1}^n w_i(Y_i,z)\Delta_i I(Y_i\le s)K_h(Y_i-t)\\
& = \frac{\sum_{i=1}^n w_i(Y_i,z)\Delta_i I(Y_i\le s)K_h(Y_i-t)}{\sum_{i=1}^n w_i(Y_i,z)\Delta_iI(Y_i\le s)}\cdot \left\{\sum_{i=1}^n w_i(Y_i,z)\Delta_i I(Y_i\le s)\right\}.
\end{align*}	
Note that the first term in the above equation is a weighted kernel density estimator for a density function $\lambda(t\mid z)/\Lambda(s\mid z)$ on $(0,s]$, and the second term, $\sum_{i=1}^n w_i(Y_i,z)\Delta_i I(Y_i\le s)$, estimates $\Lambda(s\mid z)$. Therefore, bandwidth selection for weighted kernel density estimation \citep{wu1997cross} can be applied to select $h$ in (11).''
\end{quote}
}}




\emph{The authors also claim they are using sample splitting, using a subset A to build the tree and a subset B to estimate the quantities in the terminal nodes; how is this sample splitting incorporated into the averaging of the estimating equation? The authors I believe are using B only when computing the weight and A only when building the tree, but this is brushed over rather quickly.}

\noindent{\blue{\textbf{Response}: Your understanding is correct. One subset is only used to calculate the weight and the other is only used to build the survival tree. In this revision, we have given the formulas for weight calculation based on sample splitting and added the following discussion.}}

\begin{quote}
\blue{``We note that sample-splitting is used here to grow honest trees for reduced bias. Using  the ${\mathcal I}_{1b}$ sample to place the splits and holding out the ${\mathcal I}_{2b}$ sample to do within-leaf estimation yields honest trees. The honesty condition is proven to be successful in the literature on regression forests and is required for valid statistical inference; readers are referred to \citep{wager2017estimation} for an in-depth discussion. With sample-splitting, the weight can be calculated as $$w_i(t, z) =\sum_{b=1}^B {I\left( i \in {\mathcal I}_{2b}, Z_{i}(t) \in l_b\{z \}, Y_{i}\ge t\right)}\big/B{\sum_{j=1}^n I\left(j \in {\mathcal I}_{2b}, Z_j(t)\in l_b\{z \}, Y_{j} \ge t\right)}.$$	
	Forest-based estimation using sample-splitting is given in Algorithm 2. A formal study of the theoretical properties of the proposed survival forest will be our future work.''}
\end{quote}
\emph{A clearer description of what the survival forests algorithm is doing would be helpful. The manuscript says the details are in the supplementary material, but I could not find any details there. Just to give a trivial example, random forests often do not need to be pruned, and I assume the authors are not pruning when they use random forests, but I could not find this detail discussed. An, given the smoothing, is it even possible to grow out the entire tree without a disaster occurring?}

\noindent{\blue{\textbf{Response}: To improve the presentation, we have moved the survival forest algorithm from the Appendix to the main text (please see Algorithm 2). For the specific question on pruning, we do not prune in our forest algorithm. \\Unlike other survival forest algorithm that directly average the prediction from individual trees, the hazard prediction of these large trees are \underline{not} used in our final prediction/estimation. Instead, what we do is to use a weighted training sample to estimate the hazard function, and the weights are calculated using the partition from each tree. Simulation studies shows reasonably good performance with small terminal node sizes.}}


\emph{\textbf{Other comments}}

\begin{itemize}
	\item Referee 1 and Referee 2 ask for software. This gives another avenue for the authors to address any computational concerns, as they can report timings for the software and make it available so that interested readers can try it themselves.
	
	\noindent{\blue{\textbf{Response}: Thanks for the comments. In this submission, we have included an R package \texttt{rocTree}. The package will be made available to public soon.}}
	
	\item Referee 3 wonders at the choice of ROC versus other measures of predictive accuracy, wondering why the authors do not use the Brier score. I also wonder to what extent the method the authors present is fundamentally tied up with the use of ROC curves. Does the authors approach for incorporating time dependence extend to other measures of predictive accuracy, and they just favor the use of ROC curves, or does their methodology fundamentally depend on the use of ROC curves somehow?
	
	\noindent{\blue{\textbf{Response}:  When all the covariates are time-independent, one may consider the use of Brier score. However, it is not clear how to extend the Brier score to handle time-dependent covariates. The Brier score is based on survival function, which may not be meaningful in the presence of time-dependent covariates.}}
	
	\blue{It is also worthwhile to point out that, the approach for incorporating time dependence and the martingale estimating equation based ensemble methods can be applied when other splitting criterion is used. Here we focus on ROC curve because it serves as an appropriate splitting criterion. Other splitting rule will be investigated in our future work.}
		
		
	\item Referee 3 worries that the consistency results the authors present are invalid due to the fact that the observations within each terminal node are dependent. The authors should draw more attention to this issue, and either explain why it is not a problem or correct the issue if it is.
	
	\noindent{\blue{\textbf{Response}: The result in Theorem 1 is developed for a partition-based estimator and does not incorporate the algorithm to build the tree. Under our setting, the observations within each terminal node changes over time, but the observations within one terminal node at a fixed time point are independent. For a node $\tau$, the estimator for within node hazard $\widehat{\lambda}(t\mid\tau)$ in (3) mainly requires a usual i.i.d. training sample and within node independent censoring. }}
	
	\noindent{\blue{We note that a recent work \citep{cui2017some} on random survival forests pointed out that under conditional independent censoring, the observations within a terminal node are independent but non-identically distributed, thus censoring time is not independent of survival time within the terminal node. In practice, the induced dependent censoring problem arises in most of the existing data-dependent splitting rules, so our discussion in Section 2 and 3 is under stronger censoring assumptions. In Supplementary Materials, we have added a discussion to correct the bias from potential dependent censoring, using a similar idea of estimating censoring distribution as in \cite{steingrimsson2018censoring}.}}
		
	\item Referee 3 raises some serious issues with the simulation study. In particular, that it is unfair to compare the proposed method to methods which ignore the time-varying covariates. He points out that the authors could easily compare to a time-varying Cox regression, just to have a suitable competitor.
	
	\noindent{\blue{\textbf{Response}:  In this revision, we have revised the simulation studies and added the comparison with the Cox model with time-dependent covariates.}}
		
		
	\item I found the real data analysis to be rather hurried, and would like to see an example which is more thoroughly carried out.
	
	\noindent{\blue{\textbf{Response}: To address your comment, we have included more details and the results of other commonly used methods such as CART. Due to the limit on space, we move some discussion into Supplementary Materials. }}
		
\end{itemize}

\newpage
\noindent \textbf{Response to Referee \# 1's comments:}

\emph{The authors have written an interesting article.  However, I have some
concerns about the presentation of their material.  In general,
introductory material is avoided in favor of a litany of references.
I'm not asking for a less rigorous portrayal.  Rather, I am suggesting
that the authors need to do more to introduce their topic properly to
the reader.  For example, knowledge of trees is simply assumed whereas a brief introduction to trees would be welcome.  And, they should be more careful in explaining their methodology by considering the reader's perspective and his/her likely questions.  The authors should not just assume that the reader is intimately familiar with all of the reference material.  Of course, the article can't possibly
recapitulate all previous research so choices for brevity must be
made.  But, there needs to be an intelligent balance between helping
the reader understand with content and references vs. providing
references only.
}

\noindent{\blue{\textbf{Response}:  Your comment is very much appreciated. In this revision, we have expanded the discussion of existing tree-based methods. Instead of simply listing the references, we add more details of some key works and recent works. We also make changes to the presentation of our methodology to improve readability.}}


\emph{\textbf{Major points}}

\emph{Software and data: can the authors provide the reviewers with software
to fit their models?  Is the application data publicly available?
}

\noindent{\blue{\textbf{Response}:  In this submission, we have included an R package \texttt{rocTree}. The CPCRA data is not publicly available, thus we include a simulated dataset as an example in the R package.}}


\emph{Throughout: the acronym ``ROC" is never defined.  Presumably, the
authors are referring to the Receiver Operating Characteristic; an opaque historical term which the authors should demystify, rather than merely repeat, for the reader's benefit.  Also, the mathematical/statistical/probabilistic definition of the ROC should be developed earlier; probably at the beginning of Section 2 rather than only belatedly and incompletely on the bottom of page 7.
}

\noindent{\blue{\textbf{Response}:  Thanks for the comment. We followed your suggestion and defined the acronym ``ROC'' (Receiver Operating Characteristic) in the abstract and on page 3. In this revision, we move all the discussion related to a time-invariant partition to Section 2 and all the discussion related to the ROC-guided algorithm to construct such a tree to Section 3. The definition of the original ROC curve is introduced at the beginning of the Section 3.}}

\emph{Page 4, Section 2.1: It is not clear whether $M$ is fixed or random? Is it finite?  Do we define it or does it arise from the data analysis itself?  Does it vary with time, i.e., $M(t)$ or $M^H(t)$?  Are my concerns addressed later, perhaps, in Remark 1?}

\noindent{\blue{\textbf{Response}:  In Section 2.1, $M$ is fixed, finite and does not vary with time. Section 2.1 presents the estimation of the prediction function of a fixed partition. For example, after a tree algorithm construct a partition, we can use the estimation procedure in 2.1 to calculate the partition-based risk prediction function. To address your concern, we emphasize the partition is fixed by saying
\begin{quote}
``We consider a fixed partition on ${\mathcal Z}_t$ that divides the survivor population into $M$ subgroups, denoted by ${\T}=\{ \tau_1, \tau_2, \ldots, \tau_M\}$. The partition $\T$ is time-invariant in the sense that it can be applied on ${\mathcal Z}_t$ for all $t\in (0, s]$.''
\end{quote}}}

\emph{Page 4, Section 2.1: Another issues is that the authors draw our attention to a fixed time interval $[0, t_0]$.  a) do we really want to include zero?  b) the connotation of the zero subscript is unclear; perhaps, the interval should be denoted by $(0, s]$ or $(0, b]$ where no distracting subscript is needed.  Regardless of what we call it: the right-most time point is said to be fixed.  Is the point being made that it is finite or is it something deeper?
}

\noindent{\blue{\textbf{Response}:  (a) The integrals of $\CON_t$ on $[0,t_0]$ and $(0,t_0]$ are usually the same. Since we are interested in hazard function for $t>0$, we can drop zero. \\(b) We agree that the zero subscript is not needed, thus we have changed the interval to be $(0,s]$. In this paper, we focus on the case where the right-most time point is pre-specified and thus is fixed and finite. We have clarified on page \cy{5} that ``Let $s$ be a pre-specified constant''.}}

\emph{Page 4, equation 1: It is not clear to the reader why the authors have essentially adopted two different notations for essentially the same thing.  For example the beginning of equation 1 has $\lambda_{\mathcal{T}}(t, Z(t))$ while at the end of the summation there appears $\lambda(t|\tau)$.  Although, I realize that the two $\lambda$ functions represent different aspects of the hazard.  But, my issue is: what are the authors trying to convey by switching from a comma to a bar to separate the arguments?  Explain.
}

\noindent{\blue{\textbf{Response}: In our previous manuscript, the function $\lambda_{\T}(t,z)$ is a $(1+p)$-dimensional function, thus we use a comma to separate $t$ and $z$. The node $\tau$ is  a set of $p$-dimensional vectors; a similar notation of ``$|\text{node}$'' also appears in Breiman et al. (1984) (e.g., page 123) to denote node-specific mean functions. To address your comment, we have changed $\lambda_{\T}(t,z)$ to $\lambda_{\T}(t\mid z)$  (and $\lambda(t,z)$ to $\lambda(t\mid z)$ )to make the notations more coherent.}}



\emph{Page 4, bottom of the page: The sentence ``On one hand, a time-invariant partition ..." seems to be conveying something important, but it is rather terse and unlikely to be properly appreciated by the reader.  Please elaborate more clearly.
}

\noindent{\blue{\textbf{Response}: Thanks for the suggestion. Following your suggestion, we have elaborate the partition scheme more carefully by adding a new paragraph and a figure (page \cy{5-6}). }}


\emph{Page 5, above and including equation 2: The authors use the "uc" superscript seemingly out-of-nowhere; I'm not familiar with this euphemism and I suspect many readers are not either.  Presumably, it must mean something.  The reader instinctively backtracks for the definition and is left with an uneasy feeling since no explanation can be found.  Furthermore, some wags will very obviously read this as
"Fuc" which is perhaps amusing, but most certainly not what the authors intended.  I strongly suggest "uc" be either dropped entirely or replaced with a different notation; if the latter, an explanation of the letters chosen should be provided above equation 2 so we don't have to guess.}

\noindent{\blue{\textbf{Response}: Thanks for pointing this out. We were intended to mean that the quantity is estimated using ``\textbf{u}n\textbf{c}ensored'' survival times but did not explain this notation.  We now change $F^{\rm uc}$ to $F^{\ast}$.}}

\emph{Page 6, Remark 1: Generally, a remark is an important point for the reader to know that follows from what has been shown, but would not be readily apparent without making the remark itself.  However, here I'm
not sure what point the remark is making.  If $\mathcal{T}$ is a partition of ${\mathcal Z}_t$, then what is being conveyed in this remark?  Is it saying that over time $\mathcal{T}$ is not really a partition after all (similar to my concerns about $M$)?  This apparent paradox should be explained and dispelled via the communication of the remark; furthermore, the remark should be clearly explained in words as well as mathematically for the reader's benefit.}

\noindent{\blue{\textbf{Response}: In the discussion before Remark 1, we have assumed that $\mZ_t$ does not change with $t$, thus a time-invariant partition can be applied on the whole time interval. In practice, if $\mZ_t$ depends on $t$, it is difficult to define a time-invariant partition on the time-dependent predictor space. 
As a solution, we propose to transform the predictors so that the transformed predictor space does not change with time. Then a time-invariant partition can still be used on the transformed space. We have revised Remark 1 to clarify this point.}} 

\emph{Page 8, definition of $\CON(.)$.  I take issue with calling it $\CON(.)$ since via integration of $\CON_t(.)$ you no longer have a concordance measure per se.  Rather, you have an integrated concordance measure; a better name is possibly $\ICON(.)$ which sounds better and looks nicer. Also, using $\ICON$ in reference to this measure in the text later on (such as on page 9 at the end of Section 2 and the beginning of Section 3, etc.) is more readable and appealing than CON in my opinion.}

\noindent{\blue{\textbf{Response}: Thanks for the suggestion. We agree that ICON is a great notation. We have followed your suggestion and changed CON to ICON.}} 

\emph{Page 10, definition of $CON_t$: I'm not sure how you got this
expression.  And even so, I don't understand it.  For this point, let
me define it as $CON_t(.)=\frac{a}{b}+\frac{c}{d}$.  Can it not be
more simply put as $CON_t(.)=\frac{a+0.5c}{b}$?  Also, how do the
indices of summation actually iterate for the parts $a$ and $b$?  
}

\noindent{\blue{\textbf{Response}: To address your comments, we revised the indice of summation to avoid confusion. Given ${\T} = \{ \tau_1,\tau_2,\ldots,\tau_M \}$, we have
\begin{align*}
& \CON_t(\lambda_{\T}(t\mid \cdot))  \\\nonumber
=&~ \frac{\sum_{j=1}^M\sum_{k=1}^M I\{\lambda(t\mid\tau_j)>\lambda(t\mid\tau_k)\}f^{}(t,\tau_j)S^{}(t,\tau_k)+ 0.5\sum_{j=1}^M f^{}(t,\tau_j)S^{}(t,\tau_j)}{\sum_{j=1}^M \sum_{k=1}^M  f^{}(t,\tau_j)S^{}(t,\tau_k)}.
\end{align*}}}
\noindent{\blue{Proof. Note that 
\begin{align*}
& \CON_t(\lambda_{\mathcal T}(t\mid \cdot))\\
=& P\{\lambda_{\mathcal T}(t\mid Z_1(t))>\lambda_{\mathcal T}(t\mid Z_2(t)) \mid T_2>T_1 = t\}+ 0.5 P\{\lambda_{\mathcal T}(t\mid Z_1(t))=\lambda_{\mathcal T}(t\mid Z_2(t)) \mid T_2>T_1 = t\}.\\
\end{align*}
The first term can be written as
\begin{align*}
& P\{\lambda_{\mathcal T}(t\mid Z_1(t))>\lambda_{\mathcal T}(t\mid Z_2(t)) \mid T_2>T_1 = t\}\\
= & \frac{P\{\lambda_{\mathcal T}(t\mid Z_1(t))>\lambda_{\mathcal T}(t\mid Z_2(t)), T_2>T_1 = t\}}{P(T_2>T_1 = t)}\\
= & \frac{\sum_{j=1}^M \sum_{k=1}^M I\{\lambda(t\mid \tau_j) > \lambda(t\mid \tau_k)  \} P(Z_1(t)\in\tau_j, Z_2(t)\in\tau_k, T_2>T_1 = t)}{\sum_{j=1}^M \sum_{k=1}^M P(Z_1(t)\in\tau_j, Z_2(t)\in\tau_k, T_2>T_1 = t)}\\
=& \frac{\sum_{j=1}^M \sum_{k=1}^M I\{\lambda(t\mid \tau_j) > \lambda(t\mid \tau_k)  \} f(t,\tau_j)S(t,\tau_k)}{\sum_{j=1}^M \sum_{k=1}^M  f(t,\tau_j)S(t,\tau_k)}.\\
\end{align*}
The second term can be written as
\begin{align*}
& P\{\lambda_{\mathcal T}(t\mid Z_1(t))=\lambda_{\mathcal T}(t\mid Z_2(t)) \mid T_2>T_1 = t\}\\
= & \frac{P\{\lambda_{\mathcal T}(t\mid Z_1(t))=\lambda_{\mathcal T}(t\mid Z_2(t)), T_2>T_1 = t\}}{P(T_2>T_1 = t)}\\
= & \frac{\sum_{j=1}^M \sum_{k=1}^M I\{\lambda(t\mid \tau_j) = \lambda(t\mid \tau_k)  \} P(Z_1(t)\in\tau_j, Z_2(t)\in\tau_k, T_2>T_1 = t)}{\sum_{j=1}^M \sum_{k=1}^M P(Z_1(t)\in\tau_j, Z_2(t)\in\tau_k, T_2>T_1 = t)}\\
=& \frac{\sum_{j=1}^M   f(t,\tau_j)S(t,\tau_j)}{\sum_{j=1}^M \sum_{k=1}^M  f(t,\tau_j)S(t,\tau_k)}.\\
\end{align*}
Thus the equation is proved.}}


\emph{Page 10, "Consider the partition ... on the node $\tau_j$, $\tau_j \in
\mathcal{T}$".  I believe that one $\tau_j$ will suffice to convey the
desired meaning and, furthermore, will avoid unnecessary confusion.
}

\noindent{\blue{\textbf{Response}: Without loss of generality, we have changed $\pi_j$ to $\pi_1$.}} 

\emph{Page 10, next sentence after the above.  I find the definition of
${\T}_s$ very confusing as written.  Why do you not use the
notation from Section 1.1. of the Supplmentary Material?  I have a
bigger problem which arises when we get to $s^{\mbox{opt}}=\arg\max_s
...$; see below.}

\noindent{\blue{\textbf{Response}: In this revision, we change our presentation to avoid using a notation $s$ for a split. To denote the partition after splitting, we use $\T'$ instead of ${\T}_s$. We did not use the notation of Section 1.1 in Supplementary Materials, because it is not clear which nodes are the two child nodes after splitting, and the child nodes are used in Proposition 2.}} 

\emph{Page 10, "Based on Proposition 2, ...  When ...".  After "When", there is something amiss.  I can only assume that something was mis-typeset and, therefore, the point being made is lost.
}

\noindent{\blue{\textbf{Response}: Thanks for pointing this out. We missed a ``$\widehat{\omega}$'' in the previous manuscript.}} 

\emph{Page 10, "$s^{\mbox{opt}}=\arg\max_s ...$".  So, when the $s$ subscript was introduced above it was just a mnemonic device for "splitting". However, here you appear to be using $s$ as an index.  This doesn't follow from what has come before.  Again, why was the notation of Section 1.1 of the Supplementary Material not chosen for this purpose?
}

\noindent{\blue{\textbf{Response}: To avoid misconception, we now define optimal splits by words without introducing the $s$ notation.}} 
\begin{quote}
\blue{``At each splitting step, an optimal split is chosen to maximize $\widehat{\ICON}$ among the possible splits on all of the nodes.''}
\end{quote}
\emph{Page 12, beginning of Section 3.2: The authors say, "Given a new
observation $Z_0(t)$".  However, it is not clear what is meant and the
zero subscript is no help at all.  Do they mean for a subject not in
the training set, i.e., perhaps a subject in the hold-out test set?
And why a zero subscript?  Perhaps, because it is out of the scope of
$i$?  Let's make this less mysterious for the reader.
}

\noindent{\blue{\textbf{Response}: Yes, the zero subscript means an independent observation that is not in the training set. We have revise the sentence to be ``Given a new observation $Z_0(t)$ {that is independent of the training sample}''.}} 

\emph{Page 12, equation 5: mismatched parentheses in the denominator.  
}

\noindent{\blue{\textbf{Response}: Revised.}} 

\emph{Page 13, $m (m<p)$: a good choice for $m$ is likely to be data dependent.  How is the user supposed to choose $m$ in practice with real data where the true relationships are obviously unknown? Cross-validation?  For example, on page 17, the authors appear to set $m=\sqrt{p}$ which has a nice ring to it, but the justification is unclear.
}

\noindent{\blue{\textbf{Response}: Yes, $m$ can be viewed as a tuning parameter.  $m=\sqrt{p}$ is a typical choice in random forests \citep{friedman2001elements} and was used in many existing works of survival forests \citep{zhu2012recursively,steingrimsson2018censoring}. We have indicated that ``Following many existing works in survival forests \citep{zhu2012recursively,steingrimsson2018censoring}, we randomly select $\lceil \sqrt{p}\rceil$ variables at each splitting.'' In practice, one may also consider the use of cross-validation is choosing $m$.}} 

\emph{Page 15, scenarios: it appears that the authors have deliberately refrained from considering non-proportional scenarios.  This is a serious error since the authors have turned a strength of their method into a weakness.  The methodology proposed in this article allows time-dependent covariates to be incorporated which could be manipulated to detect and model non-proportionality, can they not?
}

\noindent{\blue{\textbf{Response}: Thanks for pointing this out. We have revised the simulation scenarios and include both proportional and non-proportional scenarios.}} 

\emph{Appendix, generalized definition of time-dependent ROC: I don't understand the re-definition of $\TPR^*_t$ as taking two arguments whereas every other usage in the article only receives one argument (including its usage in the last sentence of this Appendix section). Similarly, the re-definition of $\FPR^*_t$ seems to be problematic especially when you invert its definition: is $\{\FPR^*_t\}^{-1}$ well-defined and unique?
}

\noindent{\blue{\textbf{Response}:  We have revised the Appendix to make the definition more complete. The functions $\FPR_t$, $\TPR_t$, $\ROC_t$, $\ROC_t^*$ take one argument, and $\FPR_t^*$ and $\TPR_t^*$ take two arguments. 
To clarify the definition of inverse functions, we first define the (generalized) inverse of $\FPR_t$ as $\FPR_t^{-1}(q) = \inf\{c\in {\mathbb R}: \FPR_t (c) \le q  \}.$ Then we define ${\FPR_t^*}^{-1}(q) = (c_q, \gamma_q)$, where
\begin{align*}
c_q = \FPR_t^{-1}(q), ~\gamma_q = 
\begin{cases}
\frac{q - \FPR_t(c_q)}{\FPR_t(c_q-) - \FPR_t(c_q)}, &~ \text{if } \FPR_t(c_q-) \neq \FPR_t(c_q),\\
0, &~ \text{if } \FPR_t(c_q-) = \FPR_t(c_q).
\end{cases}
\end{align*}
Finally, $\ROC_t^*(q) = \FPR_t^*(c_q, \gamma_q)$ can be written as
\begin{align*}
\TPR_t(\FPR_t^{-1}(q) ) + \frac{\{ q - \FPR_t(\FPR_t^{-1}(q)) \}\{\TPR_t(\FPR_t^{-1}(q)-)-\TPR_t(\FPR_t^{-1}(q))\} }{\FPR_t(\FPR_t^{-1}(q)-) - \FPR_t(\FPR_t^{-1}(q))}.
\end{align*}
} 

\emph{Appendix, random forests algorithm Step 2:  where does $s$ come from?  Should it not be $n$?
}

\noindent{\blue{\textbf{Response}: In the previous algorithm, $s$ is the size of the subsample and $s<n$, as we are using sub-sampling instead of boostrapping. We have revised the sentence to be ``Draw the $b$th subsamples from the training data'' because such a notation is used only once and $s$ is used in the notation of time interval $(0,s]$ in this revision.}} 

\emph{Figures and Tables: ideally, the captions should be understandable without reference to the corresponding text in the article so that they can stand alone.
}

\noindent{\blue{\textbf{Response}: We have revised the captions of the figures to make the more understandable.}} 

\emph{Figure 1: what does the asterisk stand for?  Also, these are
the transformed covariates, but what is the exact definition.
I'm assuming that it is Karnofsky score divided by 100, but
guessing should not be necessary.
}

\noindent{\blue{\textbf{Response}: The nodes with asterisk are terminal nodes. The Karnofsky score are transformed via its empirical distribution function among at-risk subjects, as described in Remark 1. We now add the following sentence on page 24.}} 
\begin{quote}
\blue{``We transform the Karnofsky score at $t$ via its empirical cumulative distribution function among at-risk subjects at $t$ and use KSC$(t)$ to denote the transformed Karnofsky score at $t$.''}
\end{quote}

\emph{Figure 2b: the hazards don't seem to be doing what I would expect. For example, subject A's Karnofsky score is rising presumably from 60, at time zero, to 90, at time 1.5; however, A's hazard of mortality goes through cycles: a drop, a rise and a drop rather than consistently dropping.  Similarly, C's score is fixed, yet C's hazard is rising.}

\noindent{\blue{\textbf{Response}: We would like to clarify that even if a lower Karnofsky score is associated with increasing hazard at all the time points, increasing Karnofsky score does not necessarily implies decreasing hazard function over time. This is because the value of hazard function $\lambda(t\mid z_t)$ not only depends on covariate value $z_t$ but also depend on $t$. However, we note that the previous statement on ``hypothetical subject'' is  inappropriate, because the Karnofsky score is an internal covariate that only exists before death, thus the curves do not lead to prediction for the individual survival experience as does the usual forest models with fixed covariate values. In this case, the proposed tree-based models are appropriate for exploring association and give useful information about the mechanism by which the treatment operates \citep{fisher1999time}.}} 

\blue{To avoid misconception, we now present the forest-based estimation of hazard given different Karnofsky score values and opportunistic infection statuses. We also revised the data example to make the discussion more rigorous.}




\emph{Supplementary Material, Section 1.1: To increment from Step 0 to $k$,
Step 0 should be associated with ${\T}_0$ instead of 1, right?
}

\noindent{\blue{\textbf{Response}: For $k\ge 1$, step $k$ creates the tree ${\T}_{k+1}$. The step 0 does not make any split. To avoid confusion, we delete step 0 and revise the algorithm. Please refer to Algorithm 1 in the revised manuscript.}} 

\emph{Supplementary Material, Section 2: the notation of $\phi_g(z)$ seems
insufficient for the development that follows.  For example, its use
in the next sentence would seem to indicate $\phi_g(t, z, c, \gamma)$
or $\phi_{g(t, .)}(z, c, \gamma)$ would be necessary; however, that
may be overly complex, but what is the alternative?  In order to fix
$\FPR^*_{.,t}(c, \gamma)=\alpha$, is there a unique solution with
respect to $(c, \gamma)$ for every fixed value of $\alpha$?
}

\noindent{\blue{\textbf{Response}: Yes, $\phi_g(z)$ depends on $(c,\gamma)$. We now revise the notation to $\phi_{g,c,\gamma}(z) = I(g(z)>c) + \gamma I(g(z)=c)$ and change the proof accordingly.}}
		
\noindent{\blue{To clarify the definition of inverse functions, we first define the (generalized) inverse of $\FPR_t$ as $\FPR_t^{-1}(\alpha) = \inf\{c\in {\mathbb R}: \FPR_t (c) \le \alpha  \}.$ Then we define ${\FPR_t^*}^{-1}(\alpha) = (c_\alpha, \gamma_\alpha)$, where
		\begin{align*}
		c_\alpha = \FPR_t^{-1}(\alpha), ~\gamma_\alpha = 
		\begin{cases}
		\frac{\alpha - \FPR_t(c_\alpha)}{\FPR_t(c_\alpha-) - \FPR_t(c_\alpha)}, &~ \text{if } \FPR_t(c_\alpha-) \neq \FPR_t(c_\alpha),\\
		0, &~ \text{if } \FPR_t(c_\alpha-) = \FPR_t(c_\alpha).
		\end{cases}
		\end{align*}
For any fixed value of $\alpha$, $c_\alpha$ is unique. If $c_\alpha$ is a discontinuous point of $\FPR_t$, $\gamma_\alpha$ is unique; if $\FPR_t$ is continuous at $c_\alpha$, $\FPR^*_t(c_\alpha,\gamma) = \alpha$ does not depend on $\gamma$, so we set $\gamma_{\alpha}$ to be zero. 
}} 

\emph{Supplementary Material, Section 3: I can't follow this development.
What is $\lambda(t)$ in this context?
}

\noindent{\blue{\textbf{Response}: $\lambda(t)$ is the marginal hazard function of $T$, defined as $\lambda(t)dt = P(T\in[t,t+dt) \mid T\ge t)$. For ${\T} = \{\tau_1,\tau_2,\ldots,\tau_M \}$, the $\ROC_{\lambda_{\T}(t\mid\cdot),t}$ is a set of $M+1$ points and the $\ROC_{\lambda_{\T}(t\mid\cdot),t}^*$ curve is made up of $M$ segments that connect these points. Moreover, $s(t\mid\tau_j) = {\lambda(t\mid\tau_j)}/{\lambda(t)}$ is the slope of the $j$th segment of the $\ROC_{\lambda_{\T}(t\mid\cdot),t}^*$ curve. Based on the definition in Section 1 of the Supplementary Materials, $s(t\mid \tau_j) = P(Z(t)\in\tau_j\mid T=t)/P(Z(t)\in\tau_j\mid T>t) = {\lambda(t\mid\tau_j)}/{\lambda(t)}$.}} 

\emph{Supplementary Material, Section 4: You define the $o^*(.)$ notation,
but not $o(.)$.  You should also define the little $o(.)$ for completeness.
}

\noindent{\blue{\textbf{Response}: The little $o(\cdot)$ is the standard little o notation. Specifically, $f(n) = o(g(n))$ means that $f(n)/g(n)\rightarrow 0$ as $n\rightarrow \infty$. We have added the definition for completeness.}} 

\emph{\textbf{Minor issues}}

\emph{Page 1, paragraph 1: missing an``and" between Breiman references.}

\noindent{\blue{\textbf{Response}: Added.}} 

\emph{Page 1, paragraph 2: The sentence starting with ``Survival tree ..."
starts awkwardly with a seeming mismatch in plurality.  Perhaps, a
better starting point is ``A single survival tree ...".
}

\noindent{\blue{\textbf{Response}: We have changed the sentence to be ``A survival tree ...''.}} \\

\emph{Page 3, end of Section 1: Another clash of plurality, ``simulations
studies" ought to be ``simulation studies".
}

\noindent{\blue{\textbf{Response}: We have changed ``simulations
		studies" to ``simulation studies".}} \\

\emph{Page 8, Remark 2: The third sentence is difficult to read.  This
could be fixed easily, such as ``Moreover, $f(t)$ is the marginal
density and $S(t)$ is the survival function of T".
}

\noindent{\blue{\textbf{Response}: We have followed your suggestion and revised the sentence.}} \\

\emph{Page 9, top of the page:  There are several instances of ``C-statistics"
which should be replaced with ``C-statistic".
}

\noindent{\blue{\textbf{Response}: We have checked and revised all the ``C-statistics" to ``C-statistic".}} \\

\emph{Page 11, ``a large tree often overfit ...": ``a large tree often overfits ..."
}

\noindent{\blue{\textbf{Response}: Revised.}} \\

\newpage
\noindent \textbf{Response to Referee \# 2's comments:}

\emph{This manuscript develops novel survival trees and survival forest algorithms for time- dependent covariates. The survival trees developed use generalized ROC curves for splitting and the survival forests create risk prediction models by averaging unbiased martingale estimating equations. Theory for the generalized ROC splitting criteria are developed and finite sample performance is evaluated using simulations and a clinical trial comparing treatments for HIV-infected patients. Overall I think this paper is well written, the developments are thorough, and I consider it a valuable addition to the literature on survival trees and forests. My mostly minor comments are below.}

\begin{enumerate}
	\item  \emph{Making code available would make the proposed method easier to use for practitioners.}
	
	\noindent{\blue{\textbf{Response}: In this submission, we have included an R package \texttt{rocTree} that implements the proposed methods. We will make the package available in the near future.}} \\
	
	\item  \emph{Is there an intuitive (or mathematical) explanation of why sample-splitting is useful for the survival forest procedure compared to following the more common procedures of calculating the splitting statistics and terminal node estimators using the same dataset?}
	
	\noindent{\blue{\textbf{Response}: Thanks for the comment. Following \cite{athey2018generalized}, the reason of using sample-splitting is to grow ``honest trees'' to reduce bias and ensure good statistical behavior \citep{wager2017estimation}. An example of an honest tree is one where the tree structure is constructed using one subsample, while the predictions at the leaves of the tree are estimated using a different subsample. As in \cite{wager2017estimation}, honest trees are required to achieve valid statistical inference (i.e., asymptotically unbiased and normal) for random forests in general. Although regression forests can be consistent even if this honesty condition does not hold, the rates of ``non-honest'' forests are asymptotically bias- instead of variance- dominated.  A formal study of theoretical properties of using honest trees in the proposed forest will be our future work. To address your comment, we have added a discussion on page 18.}} \\
	\begin{quote}
	\blue{``We note that sample-splitting is used here to grow honest trees for reduced bias. Using  the ${\mathcal I}_{1b}$ sample to place the splits and holding out the ${\mathcal I}_{2b}$ sample to do within-leaf estimation yields honest trees. The honesty condition is proven to be successful in the literature on regression forests \citep{wager2017estimation} and is required for valid statistical inference; readers are referred to \citep{wager2017estimation} for an in-depth discussion. With sample-splitting, the weight can be calculated as $$w_i(t, z) =\sum_{b=1}^B {I\left( i \in {\mathcal I}_{2b}, Z_{i}(t) \in l_b\{z \}, Y_{i}\ge t\right)}\big/B{\sum_{j=1}^n I\left(j \in {\mathcal I}_{2b}, Z_j(t)\in l_b\{z \}, Y_{j} \ge t\right)}.$$	
	Forest-based estimation using sample-splitting is given in Algorithm 2. A formal study of the theoretical properties of the proposed survival forest will be our future work.''}
	\end{quote}
\noindent{\blue{As an alternative, we include both random forests with and without sample-splitting in the R package.}} \\
		
	\item \emph{How was the weight function selected in the simulations and the data analysis?}
	
	\noindent{\blue{\textbf{Response}: Thanks for pointing this out. We used the weight function $\omega(t) = f(t)S(t)/P(T_2 > T_1, T_1<s )$ (page 10) in the simulations and data analysis, and the marginal distribution is estimated using the Kaplan-Meier estimator. On page \cy{21}, we have added } }
	\begin{quote}
		\blue{``The weight function in ICON is chosen as $\omega(t)d t = -S(t)d S(t)/P(T_2 > T_1, T_1<s )$, where $S(t)$ is estimated using the Kaplan-Meier estimator.''}
		\end{quote}
	
	\item \emph{It would be interesting to see the survival trees built using rpart and ctree using only baseline covariates for the HIV data.}
	
	\noindent{\blue{\textbf{Response}: In this revision, we have included the trees from \texttt{rpart} and \texttt{ctree} in the data analysis. Due to the space limit, we move part of the discussion to the Supplementary Materials.}} \\

	
	\item \emph{The theory in Theorem 1 is only developed for unpruned trees and does not incorporate the pruning or cross-validation steps. To the best of my knowledge, how to incorporate the pruning and cross-validation into the theoretical developments is still an open problem for the much simpler case of fully observed binary or continuous outcomes and therefore I think it is unrealistic to expect the authors to handle the case of pruned case in the much more complex settings this paper considers. I liked that the authors are honest about these limitations when discussing the implications of Theorem 1.}
	
	\noindent{\blue{\textbf{Response}: We very much appreciate your comment on the theoretical developments. The purpose of the current result is to justify the partition-based kernel type estimator. In the future work, we will explore the possibility of establishing large sample results of the algorithm-based estimator. }} \\
	
\end{enumerate}

\newpage
\noindent \textbf{Response to Referee \# 3's comments:}

\emph{The authors propose using ROC guided survival trees and forests for a
nonparametric approach to event history analysis with time dependent
covariates.  This is an interesting and important area for the tree
literature as time dependence poses unique challenges to conventional
tree building approaches.  The core idea is to utilize a time
dependent concordance based splitting rule to create a tree partition
from which one can then extract the hazard function estimate for a
specific covariate.  While I found some aspects of the paper
interesting, I have several comments for improving the work:
}

\begin{enumerate}
	\item \emph{ With all the notation, remarks, propositions and so forth, the way
	the paper is currently written obscures the main concepts and makes it
	difficult for readers to understand what is going on.}
	
	\noindent{\blue{\textbf{Response}: We appreciate your helpful comments. In this revision, we have changed the presentation to improve readability. Following the comments from all the reviewers, we revised some of the notations. We have included a table on notations in the Supplementary Materials. We also revised the propositions so that they are more self-contained. Please find the point-to-point response below.}}
		
	\item  \emph{For example, what makes time dependent covariates difficult for
	trees is that there is no unique way to assign data points to nodes.
	Even this simple point is not made clearly.  How the authors propose
	to solve this issue is also not clear.}
	
	\noindent{\blue{\textbf{Response}: In our framework, the way to assign a subject to nodes at each fixed time $t$ is unique, and one subject is allowed to enter different terminal nodes at different time points. To address your comment, we have added a new paragraph with discussion and an example for better illustration in Section 2.1, page \cy{5}.}} 
		
	\item  \emph{It appears what is being proposed is a time invariant partition
	for hazard estimation.  For example, equation (1) indicates that the
	hazard for prediction is obtained by determining the terminal node
	membership for a feature.  Thus a fixed time $t$ will be chosen, and the
	covariate evaluated at that point, and from this the estimated hazard
	obtained from the node.  A consistency argument (Theorem 1) is given
	as justification of the approach of using a time invariant partition.}
	
	\noindent{\blue{\textbf{Response}: In this paper, we considered the use of a time-invariant partition on a time interval of interest. The partition is applied on the whole interval, and is constructed using the observed data within the interval. }} \\
		
	\item \emph{However, the algorithm used depends on time $t$ and therefore there is a disconnect between what is being done in practice and the theoretical justification.}
	
	\noindent{\blue{\textbf{Response}: We would like to clarify that our algorithm constructs a partition on a time interval of interest, and splitting and pruning is based on integrated concordance (ICON) defined on the time interval (Here ICON is the revised notation for CON in the previous manuscript, in order to reflect the different between integrated $\CON_t$ and $\CON_t$ at a single time point $t$). To avoid misconception, we  add the survival tree algorithm (Algorithm 1, page \cy{15}).}} \\
		
	\item \emph{As an example of point (4) consider the bottom of page 5 and top
	of page 6 which discusses the kernel density estimator.  A classical
	bandwidth consistency argument is stated but this assumes iid data
	which is not the case here as the estimator is applied within a node
	which induces dependence.}
	
	\noindent{\blue{\textbf{Response}: We agree that for a tree with a finite number of terminal nodes, the data in one terminal nodes may not be i.i.d., if conditionally independent assumption is assumed. The reason is that censoring time can be dependent with the survival time within a node \citep{cui2017some}. Therefore, we indicated that the estimator is discussed under independent censoring within each terminal node on page \cy{6}, which is satisfied for example, under administrative censoring (censoring is not covariate-dependent).}} 
	
	\noindent{\blue{When censoring depends on the baseline covariates, we also consider an extension following \cite{steingrimsson2018censoring}, where the conditional censoring distribution is estimated and used to weight the observations. Due to the space limit, we have included the details in the Supplementary Materials.}} 
			
	\item  \emph{Section 3.1 (splitting) is perhaps the most important from the
	perspective of application, but not enough time is spent on details
	that would be useful for researchers.}
	
	\noindent{\blue{\textbf{Response}: To address your comment, we have added more details on splitting and move the algorithm to the main text.}} 
		
	\emph{(a) For example, will this be computationally demanding?  As with
	most concordance calculations they become hard to implement with large
	n because $n^2$ calculations are needed.  }
	
	\noindent{\blue{\textbf{Response}: The splitting criterion is calculated based on Equation (6). After calculating the node-specific quantities, $O(M^2)$ calculations are needed ($M$ is the number of terminal nodes in the tree) to calculate $\CON_t$. This is different from many existing concordance calculations because the tree-based prediction functions take discrete values. The computational cost also depends on number of time points used in evaluating ICON. Simulation studies show that a moderate number of time points (e.g., 10) can achieve relatively good performance. Moreover, the computational cost of our second splitting rule based on $\Delta\ICON_\tau$ (given on page \cy{13}) is less than the ICON rule. Specifically, the splitting criterion can be written as a weighted absolute difference between the hazard functions of the two child nodes thus can be easily calculated. }} 
		
	\emph{(b) Another point is that the splitting rule substitutes estimates for population values.  Do you have to estimate all these parameters for each split ``s" on the node?  How feasible is this?  Because the data is dependent in a node, how does this affect estimators like the kernel density estimator?}
	
	\noindent{\blue{\textbf{Response}: Like many other survival tree algorithms, we calculate the parameters for each split. In response to the comment ``the data is dependent in a node'', we conjecture the referee means that ``censoring is dependent within a node, while the observations are still independent'', which is discussed in a recent work by \cite{cui2017some} (Please correct us if we misunderstand this comment). Dependent censoring will generally bias the within-node estimates, and this applies to all the survival tree estimates including other work that reports the Nelson-Aalen or Kaplan-Meier estimates. Thus the kernel type estimator can also be biased. As a solution, our methods can be extended to handle conditionally independent censoring by modeling the conditional censoring distribution \citep{steingrimsson2018censoring}. Due to space limit, the details are given in Supplementary Materials.}} 	
		
	\emph{(c) The final hazard estimator is based on the data from its node. This is expected to be a small sample size.  Therefore can we really expect this to be reasonable?  Density estimation based on small sample sizes does not seem practical.}
	
	\noindent{\blue{\textbf{Response}: For survival trees, we control the node size by pruning, thus large trees whose node sizes are too small for reasonable prediction is not likely to be selected as the final tree. In this revision, we have included more details on pruning on page \cy{14}. We note that the sizes of terminal nodes generally depend on the complexity of the true hazard function $\lambda(t\mid z)$, and a small node size is not always expected. For example, if the truth is a tree with only two terminal nodes, then after pruning, the node sizes are expected to be reasonably large; if the truth is very complex and requires a large tree, we would recommend the use of forest-based method to stabilize the prediction. In practice, one can use cross-validation to determine the final model. We have added a discussion in Section 7.}}
	
	\begin{quote}
	\blue{``The proposed tree-based hazard estimators involves kernel smoothing and can result in biased estimation when the terminal nodes contain a small number of observations. To solve this issue, we control the size of the tree by pruning. Thus a extremely large tree whose node sizes are too small for reasonable prediction is not likely to be selected as the final model. In practice, if the true model is complex and a very large tree is needed to fit the data, we recommend the use of random forests for stable prediction. Cross-validation can be used to determine the final model.''}
	\end{quote}
	
	\noindent{\blue{Moreover, if the time-dependent covariates are external in the sense that its path is not affected by survival, one can also predict the survival probability without smoothing (Equation (4) and (12)). When only baseline covariates are present, treating them as time-dependent and applying the proposed method yields improved prediction in our simulation studies.}} 
		
	\emph{(d) It is not even clear how the optimal split is chosen.  It seems it is the value which optimizes the estimated integrated concordance	index.  More disscussion would be helpful.}

	\noindent{\blue{\textbf{Response}: Yes. For the ICON-based splitting rule, the optimal split is chosen as the split that maximizes the estimated integrated concordance index. For the splitting rule based on $\Delta\ICON_\tau$, the optimal split is chosen as the split that maximizes the estimated within-node ICON. Algorithm 1 (survival tree algorithm) is added to address your comment.}} 
	
	\item  \emph{The use of ROC guided splitting is not properly motivated.  Why ROC instead of using the Brier score loss?  The latter has a better theoretical justification.}
	
	\noindent{\blue{\textbf{Response}: The ROC-guided splitting rule has a few nice properties: (i) Compared to splitting criteria that based on between-node heterogeneity, ROC-guided rule is more relevant to prediction accuracy. (ii) It can be extended to handle time-dependent covariates. To our knowledge, it is not clear how Brier score can handle time-dependent covariates. For theoretical justification, the target hazard function is optimal and gives the highest ROC curve, thus the ROC curve is a reasonable measure in evaluating the prediction accuracy. Potential use of Brier score in the tree-building algorithm will be studied in our future work.}} 
	
		
	\item \emph{There are technical issues that should be looked at more
	carefully.  For example the assumption "time is independent of
	censoring".  I think you are actually assuming time is conditionally
	independent of censoring given the covariate.  Please check.}
	
	\noindent{\blue{\textbf{Response}: We made different types of censoring assumptions for ease of discussion.}}
	
	\noindent{\blue{When discussing the large-sample properties, the conditional independent censoring assumption is needed. This is consistent with the existing survival tree literature \citep{butler1989tree}.}} 
		
    \noindent{\blue{When discussing the splitting rule, we assume that censoring distribution is covariate-independent. Like many data-dependent splitting rules (e.g., log-rank statistic), the proposed splitting criteria rely on valid estimation of node-specific survival risk. A conditionally independent censoring assumption may result in dependent censoring within a node, and existing survival estimators fail to provide consistent estimation. The estimation of splitting criterion can be extended to handle covariate-dependent censoring, and the details are now given in the Supplementary Materials.}}\\
		
	\item \emph{On page 13, it is stated "However, for right censored survival
	data, averaging estimated survival or cumulative hazard functions from
	deeply grown trees may result in large bias, because the node-specific
	estimate is biased when a node contains a small number of
	observations".  This statement is incorrect or at the very least
	misleading.  Estimation based on averaging survival functions will be
	consistent (just as with any random forest that uses averaging) under
	assumptions regarding the terminal node size.  Small terminal node
	sizes are only used in practice but for consistency size must go to
	infinity in some controlled fashion (as for example in Theorem 1 of
	this paper)}
	
	\noindent{\blue{\textbf{Response}: The discussion here focused on random survival forests used in practice. To avoid misconception, we have revised the sentence to be
	\begin{quote}
		``In practice, trees in the forests are often fully grown without pruning, and the sizes of terminal nodes are small. For right-censored survival data, averaging estimated survival or cumulative hazard functions from such deeply grown trees is likely to result in un-ignorable bias.''
	\end{quote}
	We agree with the reviewer on the fact that consistency results hold under assumptions regarding the terminal node size, and the bias will converge to zero as sample size goes to infinity. But it would still be desirable to reduce the bias so that the convergence rate is faster. The proposed ensemble method is demonstrated to have improved prediction accuracy in simulation studies. A more rigorous theoretical justification will be a future work.}} \\
		
	\item \emph{The empirical results need to be improved.}
	
		
	\emph{(a) The first set of simulations should be compared to methods such as
	forests, ctree, rpart etc which do not use time dependence.}
	
		
	\emph{(b) The second set of simulations should be compared to methods that
	use time dependence.  For example at the very least results should be
	compared to time dependent cox regression.}
	
		
	\emph{(c) Prediction error should be calculated using the Brier score and
	integrated Brier score which is more in line with what is seen in the
	literature.}

		
	\emph{(d) Contrary to what the authors state, it is not difficult to
	evaluate time dependent cox prediction performance.}

	\noindent{\blue{\textbf{Response}: Your suggestion is appreciated. We agree that when all the covariates are time-independent, it is not difficult to evaluate the performance of the Cox model.  Following your suggestions, we have included the Cox model for time-dependent covariates. We also added Brier score and integrated Brier score to evaluate the accuracy for prediction using baseline covariates. }} \\
	
\end{enumerate}


\newpage
\noindent \textbf{References}

\bibliographystyle{biom} 
\bibliography{tree}




\end{document}
  
